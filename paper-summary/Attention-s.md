# [main-page](../README.md)

# [Attention Is All You Need](../papers/Attention.pdf)

## Related works
* [pseudoSeg](../papers/PSEUDOSEG.pdf), [Summary](PSEUDOSEG-s.md)

## Overview
![](images/2021-05-10_101731.png)


## Methods

## Experiments

## Contribution
* introduce the Transformer

## Questions

