# [main-page](../README.md)

# [Rethinking Pre-training and Self-training](../papers/Rethinking.pdf)

## Related works
* [pseudoSeg](../papers/PSEUDOSEG.pdf), [Summary](PSEUDOSEG-s.md)

## Overview
self-training can better leverage data augmentation than pre-training   
## Methods
* trick for self-training
![](images/2021-05-10_154510.png)

## Experiments

## Contribution
* self-training actually works
* SOTA on COCO using self-training leveraging ImageNet 

## Questions

